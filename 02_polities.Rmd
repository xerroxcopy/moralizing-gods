# !diagnostics off

# Polities
  
```{r polities}
# !diagnostics off

Vars_df <- read_csv("input/variables.csv")

polities_df <- 
  polities_pre %>% 
  filter(!NGA %in% c("Crete", "Galilee", "Middle Ganga")) %>%  # Remove new NGAs
  filter(!PolID %in% c("IdCJBun", "IdKalin")) %>%  # Remove low-coverage polities causing bugs in C. Java)
  filter(!PolID %in% c("EsHabsb", "USIllinL", "GbEmpir", "InEInCo", "InBritP", "RuYakuL", "UsIroqL")) #Remove post-colonial polities (also see below for removing post-colonial polities from NGAs with only 2 polities)

NGAs <- levels(polities_df$NGA %>% as.factor()) # warnings maaaaay be okay.
```

# Multiple Imputations 

## ConstrMI.R

`ConstrMI.R` : from 30 NGAs in `polities`, select an NGA e.g., NGAs[1] = "Big Island Hawaii", filter SCdat and polities with that NGA, 



modern version:

input:
  - `polities_df`: a tibble of polities from `polities.csv`, minus duplicate minus new NGAs,etc. A simple list of individual polities per row, with PolIdD and its start/end year. 299 x 11
  - `SCdat`: a tibble of social complexity in 19,042 x 10, created with `01_precheck.Rmd`. NGA, Polity, Variable, Value From, Value To, Date From, Date To...
  

First, one value from a normal distribution created from ranged value (denoted by "range" in `Value Note` in `SCdat`) is used as value
Then "disputed" and "uncertain" is dealt by picking one. Why not just use different values as is, embodying uncertainty?..


```{r}
# a dumb but fast and readble way to mutate...
df_disputed_uncertain<- SCdat %>%
  filter(
         `Value Note` %in% c("disputed", "uncertain")) %>% # includes both disputed and uncertain
  group_by(Polity, Variable, `Date From`, `Date To`)  %>%
  sample_n(1)  # Randomly sample from disputed and uncertain, eliminate extra rows



df_ranged <- SCdat %>%
  filter(`Value Note` == "range") %>%
  mutate(
    `Value From` = pmap(., function (`Value From`, `Value To`, ...) rnorm(n = 1, mean = (`Value From` + `Value To`) / 2, sd = abs(`Value To` - `Value From`) / (2*1.645))
    ) # seems like pmap() is needed. reference: http://yoshidk6.hatenablog.com/entry/2018/08/06/154117
  ) %>% unnest()

df_else <- SCdat %>%
  filter(!`Value Note` %in% c("disputed", "uncertain", "range"))


df <- bind_rows(df_disputed_uncertain, df_ranged, df_else)
df_min <- df %>%
  select(NGA, Polity, Variable, `Value From`, `Date From`, `Date To`) %>%
  rename(PolID = Polity,
         Value = `Value From`,
         Date = `Date From`,
         DateTo = `Date To`) %>%
  ungroup() %>%
  ungroup() %>%
  ungroup() %>%
  ungroup() # 18823 x 6

# OPTION B: seemed cooler, but apparently way slower probably because of if_else
# a <- SCdat %>% 
#   group_by(NGA) %>% 
#   split(.$`Value Note` %in% c("disputed", "uncertain")) %>% 
#   modify_at("TRUE", sample_n, 1) %>% 
#   bind_rows()
#   mutate(`Value From` = if_else(`Value Note` == "range", 
#                                 )pmap(., function (`Value From`, `Value To`, ...) rnorm(n = 1, mean = (`Value From` + `Value To`) / 2, sd = abs(`Value To` - `Value From`) / (2*1.645))),
#                                 .)) # extremely slow...
                                

# OPTION C: `case_when()` extremely slow. took 3min
# df  %>% 
# mutate(
#   value = case_when(
#     `Value Note` == "range" ~ rnorm(1, mean = (`Value From` + `Value To`) / 2, sd = abs(`Value To` - `Value From`) / (2*1.645)), 
#     TRUE ~ `Value From` # Catch all statement, equivalent to `Value Note` != "range"
#   )
# ) %>% View()

```

# construct output


Join everything together. 

input:

- `df_min`: actual values of polities' social complexity variables.
- `polities_df`: to retrieve the range of years the polities were there.
- 

```{r}

df_join <-  
  left_join(
    df_min, 
    polities_df %>% select(-NGA),
    by = "PolID"
  ) %>%  # 18,823 x 15. drop NGA because polities_df has only one NGA for a PolID, whilst some PolIDs such as AfGrBct extend to other NGAs, hence produces many NA rows.
  group_by(NGA) %>% 
  mutate(
    PolStartC = (min(Start) / 100) %>% ceiling(), # previously `tmin`. the first century the polity has already started. e.g., if 99CE, 1. 100CE, 1. 101CE, 2.
    PolEndC = (max(End) / 100) %>% floor() # previously `tmmax`. the century the polity ended.
  ) %>% 
  mutate(centuries = map2(tmin, tmax, seq)) %>% 
  unnest() %>% 
  mutate(
    t = centuries * 100,
    PolStart = PolStartC * 100,
    PolEnd = PolEndC * 100
  ) %>% # convert centuries back to years
  select(-(World.Region:centuries)) %>% # tidying columns
  filter(Start <= t & End >= t) 

df_polities <- left_join(df_join, polities_df, by = "NGA") %>% 
  select(NGA, PolID, tmin, tmax, t, Start, End) %>% 
  filter(Start <= t & End >= t)

# output %>% as_tibble() %>%  is.na(.) %>% colSums()


```


I started to think that I don't needto "construct" all these in regular timesteps... Just `spread()` the `df_min` and map2?

## spread

```{r}
df_min %>% spread(key = Variable, value = Value) %>% 

```


# construct output

```{r}
  out <- matrix(nrow=c(length(100*tmin:tmax)),ncol=(3+nrow(Vars)))
  colnames(out) <- c("NGA","PolID", "Date", Vars[,3])  # Use short names for variables
  out[,1] <- as.character(NGA)
  out[,3] <- 100*tmin:tmax
  
  for(i in 1:nrow(out)){ 
    for(j in 1:nrow(polities)){
      if( (as.numeric(out[i,3]) <= as.numeric(polities[j,5])) & 
          (as.numeric(out[i,3]) >= as.numeric(polities[j,4])) ){ 
        out[i,2] <- as.character(polities[j,3]) 
      }}}
  out <- out[is.na(out[,2])==FALSE,]   # Eliminate centuries for which a polity is lacking
  
  # First populate 'out' with data tied to polities, not dates
  for(ivar in 1:nrow(Vars)){
    datV <- datSC[(datSC$Variable==Vars[ivar]) & (datSC$Date==""),]
    if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}
    for(i in 1:nrow(datV)){
      for(j in 1:nrow(out)){
        if(nrow(datV) != 0){
          if(out[j,2] == datV$PolID[i]){out[j,ivar+3] <- datV$Value[i]
          }}}}}
  
  # Next populate 'out' with data tied to a single date
  for(ivar in 1:nrow(Vars)){
    datV <- datSC[((datSC$Variable==Vars[ivar]) & (datSC$Date!="") & (datSC$DateTo=="")),]
    if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}
    for(i in 1:nrow(datV)){
      for(j in 1:nrow(out)){
        if(nrow(datV) != 0){
          century <- 100*round(as.numeric(datV[i,4])/100)
          if(out[j,3] == as.character(century)){out[j,ivar+3] <- datV$Value[i]
          }}}}}
  
  
  # Finally populate 'out' with data tied to a range of dates
  for(ivar in 1:nrow(Vars)){
    datV <- datSC[((datSC[,2]==Vars[ivar]) & (datSC[,4]!="") & (datSC[,5]!="")),]
    if(is.null(nrow(datV))){datV <- array(datV,c(1,5))}
    for(i in 1:nrow(datV)){
      for(j in 1:nrow(out)){
        if(nrow(datV) != 0){
          century <- as.numeric(out[j,3])
          tmin <- as.numeric(datV[i,4])
          tmax <- as.numeric(datV[i,5])
          if(century >= tmin & century <= tmax){out[j,ivar+3] <- datV[i,3]
          }}}}}
  
  # Calculate the proportion of data coded by century
  PropCoded <- array(0,c(nrow(out),2))
  PropCoded[,1] <- out[,3]
  colnames(PropCoded) <- c("Date1","PropCoded")
  for(i in 1:nrow(out)){
    j <- 0  
    for(ivar in 1:nrow(Vars[1:51,])){
      if(is.na(out[i,ivar+3])){j <- j+1} 
    }
    PropCoded[i,2] <- 0.1*round((nrow(Vars[1:51,]) - j)/nrow(Vars[1:51,])*1000) # Keep 3 sign digit; NB: THIS CODES ONLY PROPORTION OF SOCIAL COMPLEXITY [ROWS 1:51] FOR CONSISTENCY s
  }
  out <- cbind(out[,1:3],PropCoded[,2],out[,4:(nrow(Vars)+3)])
  colnames(out) <- c("NGA","PolID", "Date", "PropCoded", Vars[,3])  
  output <- rbind(output,out)
}  # Closing the iNGA loop

output <- output[output[,4] != 0,]  # Remove rows of missing values
write.csv(output, file="MIoutput.csv",  row.names=FALSE)

rm(dat_temp,dat,datSC,datV,out,PropCoded,century,i,iNGA,ivar,j,mean,NGA,sd,tmax,tmin,value)
```

```{r}

nrep <- 20
ImpDatRepl <- matrix(NA, nrow=0, ncol=0) 
for(irep in 1:nrep){
  print(irep)
  source("ConstrMI.R")
  source("AggrMI.R")
  source("ImputeMI.R")
  ones <- matrix(data=1,nrow=length(AggrDat[,1]),ncol=1)
  colnames(ones) <- "irep"
  ImpDat <- cbind(AggrDat[,1:4],ImpDat,(ones*irep),AggrDat[,14:32])
  ImpDatRepl <- rbind(ImpDatRepl,ImpDat)
}
```


Remove polity-dates that didn't yield 20 repl and post-colonial polities that couldn't be removed from multiple imputation due to bugs with only 1 polity/NGA

```{r}

polities <- read.csv('polities.csv', header=TRUE)
polities <- polities[polities$PolID != "InGaroL",] # removing here because it caused bugs earlier
write.csv(polities, file="polities.csv",  row.names=FALSE) 
polities <- polities[polities$PolID != "CnHChin",] # removing here because it caused bugs earlier
write.csv(polities, file="polities.csv",  row.names=FALSE) 
polities <- polities[polities$PolID != "PgOrokL",] # removing here because it caused bugs earlier
write.csv(polities, file="polities.csv",  row.names=FALSE) 

ImpDatRepl <- ImpDatRepl[ImpDatRepl$PolID != "InGaroL",] # removing here because it seemed to create bugs when you have only 1 polity in an NGA, so couldn't remove earlier
ImpDatRepl <- ImpDatRepl[ImpDatRepl$PolID != "CnHChin",] # removing here because it seemed to create bugs when you have only 1 polity in an NGA, so couldn't remove earlier
ImpDatRepl <- ImpDatRepl[ImpDatRepl$PolID != "PgOrokL",] # removing here because it seemed to create bugs when you have only 1 polity in an NGA, so couldn't remove earlier



dat_temp <- ImpDatRepl
for(i in 1:nrow(polities)){
  dat <- ImpDatRepl[as.character(ImpDatRepl[,2])==as.character(polities[i,2]),]
  if(nrow(dat)!=0){
    Time <- unique(dat$Time)
    for(j in 1:length(Time)){
      dt <- dat[dat$Time==Time[j],]
      if(nrow(dt) != nrep){
        print(nrow(dt))
        print(dt[1,1:3])
        dat_temp[as.character(dat_temp$PolID)==as.character(dat$PolID[1]) & dat_temp$Time==Time[j],14] <- -99999
      }
    }
  }
}
ImpDatRepl <- dat_temp[dat_temp$irep!=-99999,]



write.csv(ImpDatRepl, file="ImpDatRepl.csv",  row.names=FALSE)
#  end of the new scrape section
```



