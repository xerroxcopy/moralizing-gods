# !diagnostics off

# Polities
  
```{r polities}
# !diagnostics off
polities_df <- 
  polities_pre %>% 
  filter(!NGA %in% c("Crete", "Galilee", "Middle Ganga")) %>%  # Remove new NGAs
  filter(!PolID %in% c("IdCJBun", "IdKalin")) %>%  # Remove low-coverage polities causing bugs in C. Java)
  filter(!PolID %in% c("EsHabsb", "USIllinL", "GbEmpir", "InEInCo", "InBritP", "RuYakuL", "UsIroqL")) #Remove post-colonial polities (also see below for removing post-colonial polities from NGAs with only 2 polities)
# 299 x 11

NGAs_tidy <- levels(polities_df$NGA %>% as.factor()) # warnings maaaaay be okay.
```

# Multiple Imputations 

## ConstrMI.R

`ConstrMI.R` : from 30 NGAs in `polities`, select an NGA e.g., NGAs[1] = "Big Island Hawaii", filter SCdat and polities with that NGA, 

modern version:

input:
  - `polities_df`: a tibble of polities from `polities.csv`, minus duplicate minus new NGAs,etc. A simple list of individual polities per row, with PolID and their start/end year. 299 x 11
  - `SCdat_df`: a tibble of social complexity in 19,042 x 10, created with `01_precheck.Rmd`. NGA, Polity, Variable, Value From, Value To, Date From, Date To...
  

First, one value from a normal distribution created from ranged value (denoted by "range" in `Value Note` in `SCdat`) is used as value
Then "disputed" and "uncertain" is dealt by picking one. Why not just use different values as is, embodying uncertainty?..

```{r}

# a dumb but fast and readble way to mutate...
df_disputed_uncertain<- SCdat_df %>%
  filter(
         `Value Note` %in% c("disputed", "uncertain")) %>% # includes both disputed and uncertain
  group_by(Polity, Variable, `Date From`, `Date To`)  %>%
  sample_n(1)  # Randomly sample from disputed and uncertain, eliminate extra rows

df_ranged <- SCdat_df %>%
  filter(`Value Note` == "range") %>%
  mutate(
    `Value From` = pmap(., function (`Value From`, `Value To`, ...) rnorm(n = 1, mean = (`Value From` + `Value To`) / 2, sd = abs(`Value To` - `Value From`) / (2*1.645))
    ) # seems like pmap() is needed. reference: http://yoshidk6.hatenablog.com/entry/2018/08/06/154117
  ) %>% unnest()

df_else <- SCdat_df %>%
  filter(!`Value Note` %in% c("disputed", "uncertain", "range"))

df <- bind_rows(df_disputed_uncertain, df_ranged, df_else)
df_min <- df %>%
  select(NGA, Polity, Variable, `Value From`, `Date From`, `Date To`) %>%
  rename(PolID = Polity,
         Value = `Value From`,
         Date = `Date From`,
         DateTo = `Date To`) %>%
  ungroup() %>%
  ungroup() %>%
  ungroup() %>%
  ungroup() 

# OPTION B: seemed cooler, but apparently way slower probably because of if_else
# a <- SCdat %>% 
#   group_by(NGA) %>% 
#   split(.$`Value Note` %in% c("disputed", "uncertain")) %>% 
#   modify_at("TRUE", sample_n, 1) %>% 
#   bind_rows()
#   mutate(`Value From` = if_else(`Value Note` == "range", 
#                                 )pmap(., function (`Value From`, `Value To`, ...) rnorm(n = 1, mean = (`Value From` + `Value To`) / 2, sd = abs(`Value To` - `Value From`) / (2*1.645))),
#                                 .)) # extremely slow...
                                

# OPTION C: `case_when()` extremely slow. took 3min
# df  %>% 
# mutate(
#   value = case_when(
#     `Value Note` == "range" ~ rnorm(1, mean = (`Value From` + `Value To`) / 2, sd = abs(`Value To` - `Value From`) / (2*1.645)), 
#     TRUE ~ `Value From` # Catch all statement, equivalent to `Value Note` != "range"
#   )
# ) %>% View()
dim(df_min) # 18823 x 6
df_min %>% filter(PolID == "Hawaii2") # prof priest available
df_min %>% filter(NGA == "Kansai", Variable =="Social Scale Polity Population")
```

# construct output

Join everything together. 

input:

- `df_min`: a tibble, actual values of polities' social complexity variables.
- `polities_df`: a tibble, to retrieve the range of years the polities were there.
- 


join by `NGA` AND `PolID`, because `polities_df` has only one `NGA` for a `PolID`, whilst some `PolID`s such as `AfGrBct` extend to other `NGA`s, hence produces many `NA` rows.

- `PolStartC`: previously `tmin`. the first century (C) the polity has already started. e.g., if 99CE, 1. 100CE, 1. 101CE, 2.
- `PolEndC`: previously `tmax`. The century the polity ended.
- `centuries`: a sequence from `PolStartC` to `PolEndC`, e.g., if `PolStartC` = 3 and `PolEndC` = 18, 3:8. needs `unnest()`ing for tidy dataframe.
```{r}

df_join <-  
  inner_join(
    df_min, 
    polities_df,
    by = c("NGA", "PolID")
  ) %>%  # 18,823 x 15. 
  group_by(PolID) %>% 
  mutate(
    PolStartC = (min(Start) / 100) %>% ceiling(), 
    PolEndC = (max(End) / 100) %>% floor()
  ) %>% 
  mutate(centuries = map2(PolStartC, PolEndC, seq)) %>% 
  unnest() %>% 
  mutate(
    # convert centuries back to years
    t = centuries * 100,
    PolStart = PolStartC * 100,
    PolEnd = PolEndC * 100
  ) %>% 
  select(-(World.Region:centuries)) %>% 
  ungroup() %>% # ungroup required before filtering
  filter(t >= Start & t <= End) # filter overcopied rows out of polity's existing range
  # filter(pmap(., function (t, Start, End, ...) between(t, Start, End))) %>% # alternative, not working 
```

## filter duplicates

filter out if t is not close to Date, for single date datapoints, OR(|, do not use ||)ã€€filter out if t is out of Date-DateTo range, for ranged date datapoints. This line is very long because consecutive `filter()`s are `and`, not `or`. Note `!is.na(Date)` isn't quite needed in the second argument because it's always `TRUE` when `!is.na(DateTo)`, but it's there for readability.

```{r}
df_join2 <- 
  df_join %>% 
  filter((!is.na(Date) & is.na(DateTo) & (round(Date, -2) == t)) | (!is.na(Date) & !is.na(DateTo) & t >= Date & t <= DateTo) | is.na(Date)) %>% 
  group_by(NGA, PolID, Variable, t) %>% 
  filter(is.na(DateTo) | DateTo == max(DateTo)) %>% # filter entries ranging  1000-1200 and 1200-1500 which result in two entries in t=1200. e.g., duplicate t = 1500 in Hawaii2 ProfPriest
  filter(is.na(Date) | Date == max(Date)) %>% # filter rounding duplicates e.g., t=1200 JpKamak Social Scale Population of the largest settlement 1250 rounded to 1200
  ungroup() %>% 
  ungroup() %>% 
  ungroup() %>% 
  ungroup() %>% 
  select(-(Date:End), -PolStart, -PolEnd)

dim(df_join2) # ~37844 x 12
df_join2 %>% filter(PolID == "Hawaii2") # inspectingoverlapping range:  no duplicates
df_join2 %>% filter(PolID == "JpKamak") #
```



# construct output

social complexity variable names in `df_join` (`df_join$Variables`) are joined name of subsection and variable in `Vars_df`.

```{r}

# 
Vars_df_minimum <- 
  Vars_df %>% 
  mutate(Variable = paste(Subsection, Variable)) %>% 
  select(Variable, ShortName)
```


now join them together 

```{r}
df_join_shortname <-
  left_join(df_join2, 
            Vars_df_minimum, 
            by = "Variable") %>% 
  mutate(Variable = ShortName,
         ShortName = NULL) # 37854 * 12

df_join_shortname %>% filter(PolID == "Hawaii2", Variable == "ProfPriest") # ProfPriest available

df_join %>% filter(NGA == "Kansai", t == 1200) # duplicated
df_join2 %>% filter(NGA == "Kansai", t == 1200) # not duplicated
df_join_shortname[720:721, ]
output_df %>% filter(NGA == "Kansai", Date == 1200)
df_join_shortname %>% filter(NGA == "Kansai", t == 1200)
df_join_shortname[18434:18436,]


```

then `spread()`.

```{r}
df_spread <-
  df_join_shortname %>% 
  spread(key = Variable, value = Value) %>% 
  arrange(NGA, t) 
dim(df_spread)# 820 * 77. output's got 817 * 98...
```

### PropCoded

Now we want to calculate `PropCoded`, the proportion of variables actually recorded out of the 51 Social Complexity variables in `Vars_df`. For example, when `NGA == "Deccan"`, `PolID == "InSataL"` in `t == 100` has got 24 `NA` SC variables, PropCoded = (51 - 24) / 51.

```{r}
PropCoded_col <- df_spread %>% 
  select(
    Vars_df %>% 
      filter(Section == "Social Complexity variables", # there are other columns other than SC variables, e.g., freqs in `df_spread`.
             ShortName != "HeredStatus" # HeredStatus is in Vars_df, but none are recorded.
      ) %>% 
      .$ShortName 
  ) %>% 
  transmute(PropCoded = (ncol(.) - ( is.na(.) %>% rowSums())) / ncol(.)
  ) 

df_spread_prop <- 
  bind_cols(df_spread, PropCoded_col) %>% 
  select(NGA, PolID, PropCoded, everything()) # https://github.com/tidyverse/dplyr/issues/1188

```

## Verify PropCoded

Verify `PropCoded` in the paper is identical (or at least close enough) to the ones in my code.


`output` made with ConstrMI.R and `df_join` made here should be loosely identical, apart from the followings: 

1. minor `sample_n(1)` sampling errors, 
1. variable dispersion with `rnorm()`, and 
1. recoding corrections.

first, we need to create `output` that's identical with the original. to do so, Vars should be modified as done in `!MoralizingGods.R`.

```{r}
Vars <- as.matrix(read.csv('input/variables.csv', header=TRUE))
Vars <- Vars[Vars[,6]==Section1 | Vars[,6]==Section2 | Vars[,6]==Section3,] # Reduce the variables list to the Section set above

Vars[,1] <- paste(Vars[,2],Vars[,1])
```

Now run `ConstrMI.R` and come back.

tibblize `output`
```{r}
output_df <- output %>% 
  as_tibble %>% 
  mutate(
    Date = as.numeric(Date),
    PropCoded = as.numeric(PropCoded)
  )
dim(output_df) # 817 x 98
# minimal. https://community.rstudio.com/t/drop-all-na-columns-from-a-dataframe/5844/3 
output_df_min <- output_df %>% 
  select_if(
    function(x) {!all(is.na(x))
    }
  ) 
dim(output_df_min) # 817 x 71. 27 columns contain nothing
dim(df_spread_prop) # 820 x 78. while `output` got 4 columns that aren't SC variables, `df_spread_prop` got 11. 78 - 11 = 71 -  4 
colnames(output_df_min)
colnames(df_spread_prop[12:78])

# reorder SC variable columns to match `output`

df_spread_prop_reordered <- df_spread_prop %>% 
  select(NGA:PolEnd, colnames(output_df_min)[5:71])
colnames(df_spread_prop_reordered)


# profPriest in Hawaii2 t= 1200 is dropped in df_spread_prop_reordered, while it's available in output_df_min at 0.1.
View(df_spread_prop_reordered)
View(output_df_min)
```

### Visualize


```{r}
df_inspect <- df_join2 %>% 
  filter(NGA == "Sogdiana", Variable == "Bureaucracy characteristics Full-time bureaucrats")
df_inspect2 <- df_join2 %>% 
  filter(NGA == "Deccan")

df_inspect %>% 
  ggplot(aes(x = t, y = Value, colour = PolID)) +
  geom_point() +
  geom_line()
```

Ideally, points should be all lined up to `y = x`.


```{r}
left_join(
  df_spread_prop %>% select(t, NGA, PolID, PropCoded), 
  output_df %>% select(Date, NGA, PolID, PropCoded),
  by = c("t" = "Date", "NGA", "PolID")
) %>% #View()
  ggplot(aes(x = PropCoded.x, y = PropCoded.y / 100)) +
  geom_count(alpha = 0.3) +
  scale_size_area() +
  geom_abline(slope = 1, intercept = 0) +
  coord_fixed()

# some entries don't fit. why.
df_spread_prop %>% filter(NGA == "Kansai") %>% View()
output_df %>% filter(NGA == "Kansai") %>% View()

``` 


inspect by table:

```{r}
full_join(output_df %>% select(NGA, PolID, Date)), 
          df_spread %>% select(NGA, PolID, t),
          by = c("Date" = "t", "PolID")) 
Vars_df %>% filter(Section == "Social Complexity variables") %>% arrange(ShortName) %>% View()
```

The `df_spread` got 3 extra rows not contained in `output`: IrQajar and TrOttm5 for `t` = 1800 and 1900. ISSUE.


inspect by plot:

```{r fig.height = 20, fig.width = 20}
output_df %>% 
  filter(NGA == "Deccan") %>%
  gather(key = Variable, value = Value, -PolID, -NGA, -Date, -PropCoded) %>% 
  ggplot(aes(x = Date, y = Value, colour = PolID)) +
  geom_point() +
  geom_line() +
  scale_colour_viridis_d() +
  facet_wrap(~Variable, scale = "free_y") 

df_join_short %>% filter(NGA == "Deccan") %>%
  ggplot(aes(x = t, y = Value, colour = PolID)) +
  geom_point() +
  geom_line() +
  scale_colour_viridis_d() +
  facet_wrap(~Variable, scale = "free_y") 
  # theme_tufte()

```



## Aggregate

```{r}

```


```{r}

nrep <- 20
ImpDatRepl <- matrix(NA, nrow=0, ncol=0) 
for(irep in 1:nrep){
  print(irep)
  source("ConstrMI.R")
  source("AggrMI.R")
  source("ImputeMI.R")
  ones <- matrix(data=1,nrow=length(AggrDat[,1]),ncol=1)
  colnames(ones) <- "irep"
  ImpDat <- cbind(AggrDat[,1:4],ImpDat,(ones*irep),AggrDat[,14:32])
  ImpDatRepl <- rbind(ImpDatRepl,ImpDat)
}
```

Remove polity-dates that didn't yield 20 repl and post-colonial polities that couldn't be removed from multiple imputation due to bugs with only 1 polity/NGA

```{r}

polities <- read.csv('polities.csv', header=TRUE)
polities <- polities[polities$PolID != "InGaroL",] # removing here because it caused bugs earlier
write.csv(polities, file="polities.csv",  row.names=FALSE) 
polities <- polities[polities$PolID != "CnHChin",] # removing here because it caused bugs earlier
write.csv(polities, file="polities.csv",  row.names=FALSE) 
polities <- polities[polities$PolID != "PgOrokL",] # removing here because it caused bugs earlier
write.csv(polities, file="polities.csv",  row.names=FALSE) 

ImpDatRepl <- ImpDatRepl[ImpDatRepl$PolID != "InGaroL",] # removing here because it seemed to create bugs when you have only 1 polity in an NGA, so couldn't remove earlier
ImpDatRepl <- ImpDatRepl[ImpDatRepl$PolID != "CnHChin",] # removing here because it seemed to create bugs when you have only 1 polity in an NGA, so couldn't remove earlier
ImpDatRepl <- ImpDatRepl[ImpDatRepl$PolID != "PgOrokL",] # removing here because it seemed to create bugs when you have only 1 polity in an NGA, so couldn't remove earlier

dat_temp <- ImpDatRepl
for(i in 1:nrow(polities)){
  dat <- ImpDatRepl[as.character(ImpDatRepl[,2])==as.character(polities[i,2]),]
  if(nrow(dat)!=0){
    Time <- unique(dat$Time)
    for(j in 1:length(Time)){
      dt <- dat[dat$Time==Time[j],]
      if(nrow(dt) != nrep){
        print(nrow(dt))
        print(dt[1,1:3])
        dat_temp[as.character(dat_temp$PolID)==as.character(dat$PolID[1]) & dat_temp$Time==Time[j],14] <- -99999
      }
    }
  }
}
ImpDatRepl <- dat_temp[dat_temp$irep!=-99999,]

write.csv(ImpDatRepl, file="ImpDatRepl.csv",  row.names=FALSE)
#  end of the new scrape section
```

